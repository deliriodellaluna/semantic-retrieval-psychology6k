{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Xqguc04yEO"
      },
      "source": [
        "# **Psychology-6k Semantic Retrieve:Il match semantico delle risposte in psicologia**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il progetto sviluppato realizza un **sistema di retrieval semantico** capace di associare un input (domanda) all‚Äôoutput corretto (risposta) all‚Äôinterno del dataset Psychology-6k, disponibile su HuggingFace.\n",
        "\n",
        "Il flusso √® organizzato come pipeline:\n",
        "\n",
        "* **Preparazione e pulizia dei testi** ‚Äì sono state testate due modalit√†: una light (normalizzazione di base) e una con lemmatizzazione tramite SpaCy. Quest‚Äôultima non era strettamente necessaria per il progetto, ma √® stata integrata a fini didattici.\n",
        "\n",
        "* **Trasformazione in embeddings** ‚Äì tramite il modello thenlper/gte-small di SentenceTransformers.\n",
        "\n",
        "* **Calcolo della similarit√† coseno** ‚Äì per stimare la vicinanza semantica tra domande e risposte candidate.\n",
        "\n",
        "* **Ranking e valutazione** ‚Äì con la metrica Mean Reciprocal Rank (MRR), che misura la posizione media della risposta corretta nella classifica.\n",
        "\n",
        "* **Interfaccia interattiva (Gradio)** ‚Äì per esplorare le domande, confrontare le due pipeline e visualizzare i risultati.\n",
        "\n",
        "*Risultati*\n",
        "Il sistema ha ottenuto valori di **MRR molto elevati: ‚âà0.97** con la pipeline light e ‚âà0.96 con la pipeline lemma, confermando che la risposta corretta viene classificata quasi sempre al primo posto."
      ],
      "metadata": {
        "id": "vREySkrDWRtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yQEaWCa5JuB"
      },
      "source": [
        "##Caricamento Dataset\n",
        "Ho caricato il dataset **Psychology-6k** direttamente da HuggingFace (`samhog/psychology-6k`).\n",
        "\n",
        "**Dettagli del dataset:**\n",
        "- **Modalities:** Text  \n",
        "- **Format:** JSON  \n",
        "- **Size:** 1K ‚Äì 10K  \n",
        "- **Librerie utilizzabili:** ü§ó Datasets, üêº Pandas\n",
        "\n",
        "In questa fase verifico la struttura dei dati e preparer√≤ il DataFrame per l‚Äôanalisi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAzuGNJA7IZW"
      },
      "outputs": [],
      "source": [
        "#Caricamento dataset da HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Carico il dataset direttamente da HuggingFace\n",
        "dataset = load_dataset(\"samhog/psychology-6k\")\n",
        "\n",
        "# Converto la parte 'train' in DataFrame Pandas per analisi\n",
        "import pandas as pd\n",
        "df = dataset[\"train\"].to_pandas()\n",
        "\n",
        "# Mostro le prime righe\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSrMvJiMhWrE"
      },
      "outputs": [],
      "source": [
        "#  Analisi Preliminare (EDA )\n",
        "\n",
        "# 1. Conteggio di record totali\n",
        "print(\"Numero di record totali:\", len(df))\n",
        "\n",
        "# 2. Verifico se il dataset ha split (train/valid/test)\n",
        "print(\"Split presenti:\", dataset.keys())  # es. ['train'] oppure ['train','test']\n",
        "\n",
        "# 3. Valori mancanti (NaN o stringhe vuote)\n",
        "print(\"\\nValori mancanti per colonna:\")\n",
        "print(df.isna().sum())\n",
        "print(\"\\nEsempi di domande vuote:\", df[df[\"input\"].isna()])\n",
        "\n",
        "# 4. Duplicati\n",
        "dup_questions = df.duplicated(subset=[\"input\"]).sum()\n",
        "print(\"\\nDomande duplicate:\", dup_questions)\n",
        "\n",
        "dup_q_and_ans = df.duplicated(subset=[\"input\", \"output\"]).sum()\n",
        "print(\"Coppie domanda-risposta duplicate:\", dup_q_and_ans)\n",
        "\n",
        "# 5. Distribuzione lunghezze (in caratteri)\n",
        "df[\"q_len_char\"] = df[\"input\"].astype(str).str.len()\n",
        "print(\"\\nStatistiche lunghezze domande (caratteri):\")\n",
        "print(df[\"q_len_char\"].describe())\n",
        "\n",
        "# Distribuzione lunghezze (in token/parole)\n",
        "df[\"q_len_tokens\"] = df[\"input\"].astype(str).str.split().apply(len)\n",
        "print(\"\\nStatistiche lunghezze domande (token):\")\n",
        "print(df[\"q_len_tokens\"].describe())\n",
        "\n",
        "# Visualizzo un campione di domande molto corte o molto lunghe\n",
        "print(\"\\nDomande corte:\")\n",
        "print(df[df[\"q_len_tokens\"] < 3][\"input\"].head())\n",
        "print(\"\\nDomande lunghe:\")\n",
        "print(df[df[\"q_len_tokens\"] > 30][\"input\"].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJOEvJ1ju6s"
      },
      "source": [
        "### Analisi Esplorativa del Dataset (EDA)\n",
        "\n",
        "L‚Äôanalisi esplorativa del dataset *Psychology-6k* ha mostrato quanto segue:\n",
        "\n",
        "- **Dimensione**: 5846 record, tutti nello split `train` (nessuna divisione train/test predefinita).  \n",
        "- **Qualit√† dei dati**: nessun valore mancante o domanda vuota, dati quindi completi.  \n",
        "- **Duplicati**: circa 1489 domande duplicate (25%). Questo indica che un quarto delle domande si ripete, probabilmente per coprire pi√π formulazioni di casi simili. Tale ridondanza potrebbe influenzare la fase di retrieval, semplificando eccessivamente la ricerca della risposta.  \n",
        "- **Distribuzione lunghezze**: domande di lunghezza media (‚âà13 token), con minimo 4 e massimo 43 token. La variabilit√† √® contenuta, con poche domande molto lunghe che descrivono situazioni complesse (es. depressione, ansia, dipendenze).  \n",
        "- **Struttura**: il dataset non fornisce direttamente una lista di **risposte candidate**. Sono presenti solo la **domanda** (`input`) e la **risposta corretta** (`output`). Per la fase di retrieval sar√† quindi necessario costruire un set di risposte candidate, ad esempio campionando da altre risposte del dataset o seguendo le linee guida fornite dal docente.\n",
        "\n",
        "**Conclusione:**  \n",
        "Il dataset presenta una buona qualit√† complessiva: √® completo, ben bilanciato nelle lunghezze e senza valori nulli. Gli aspetti critici principali sono:  \n",
        "1. la presenza di domande duplicate (‚âà25%), che richiede attenzione in fase di valutazione;  \n",
        "2. l‚Äôassenza di candidate answers predefinite, che dovranno essere generate artificialmente per testare il sistema di retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HasCKEvh_tX7"
      },
      "source": [
        "\n",
        "## **Pulizia e normalizzazione del testo**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nA3dcCiBbrW"
      },
      "source": [
        "Tecniche di pre-processing utilizzate\n",
        "\n",
        "In questa fase ho applicato due tipi di pre-processing:\n",
        "\n",
        "1. **Pulizia leggera**  \n",
        "   Ho normalizzato il testo portandolo in minuscolo, eliminando caratteri rumorosi (emoji e simboli non alfanumerici) e compattando gli spazi.  \n",
        "   Questa scelta serve a rendere i dati pi√π uniformi senza alterare troppo il contesto, cosa utile per modelli di tipo SBERT che beneficiano di frasi naturali.\n",
        "\n",
        "2. **Lemmatizzazione con SpaCy + filtro POS**  \n",
        "   Ho usato SpaCy per ridurre le parole al loro lemma (es. *running*, *ran* ‚Üí *run*) e ho mantenuto solo le parole di contenuto (sostantivi, verbi, aggettivi e nomi propri).  \n",
        "   In questo modo riduco la variabilit√† morfologica e mi concentro sugli elementi pi√π significativi delle frasi.\n",
        "\n",
        "Ho quindi applicato **due tecniche linguistiche** (lemmatizzazione e POS tagging).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fREWn-l1wYMe"
      },
      "outputs": [],
      "source": [
        "# Pulizia e normalizzazione del testo\n",
        "#importo la libreria\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# In questa parte definisco due modalit√† di pulizia:\n",
        "# - pulizia leggera: normalizza il testo senza stravolgerlo (pi√π adatta per embeddings)\n",
        "# - lemmatizzazione: riduce le parole alla radice, cos√¨ riduco la variabilit√† morfologica\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# PULIZIA LEGGERA\n",
        "# -----------------------------\n",
        "# Qui creo una whitelist di caratteri che voglio mantenere:\n",
        "# lettere, numeri, spazi e la punteggiatura pi√π comune (? ! . , ecc.)\n",
        "_NOISE_CHARS = r\"[^a-zA-Z0-9\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\'\\\"]\"\n",
        "\n",
        "def clean_text_light(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Pulizia leggera del testo:\n",
        "    - converto tutto in minuscolo\n",
        "    - tolgo caratteri strani (emoji, simboli non utili)\n",
        "    - compatto gli spazi\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(_NOISE_CHARS, \" \", text)   # tolgo caratteri non ammessi\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # elimino spazi multipli\n",
        "    return text\n",
        "\n",
        "# Applico la pulizia leggera alle colonne input (domanda) e output (risposta corretta)\n",
        "df[\"input_clean\"]  = df[\"input\"].apply(clean_text_light)\n",
        "df[\"output_clean\"] = df[\"output\"].apply(clean_text_light)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# LEMMATIZZAZIONE (OPZIONALE)\n",
        "# -----------------------------\n",
        "# In questa parte uso SpaCy per portare le parole al loro lemma (radice).\n",
        "# Esempio: \"running\", \"runs\", \"ran\" diventano \"run\".\n",
        "# Lo faccio solo su sostantivi, verbi, aggettivi e nomi propri, perch√© sono le parole di contenuto.\n",
        "# Nota: questa fase √® pi√π lenta e non sempre migliora gli embeddings contestuali.\n",
        "\n",
        "# Carico il modello di SpaCy (se non c‚Äô√®, devo scaricarlo con: !python -m spacy download en_core_web_sm)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "except OSError:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "\n",
        "KEEP_POS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}  # i POS che considero utili\n",
        "\n",
        "def lemmatize_spacy(text: str, remove_stop: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Lemmatizzazione con filtro POS:\n",
        "    - mantengo solo le parole di contenuto (sostantivi, verbi, aggettivi, nomi propri)\n",
        "    - tolgo le stopwords se richiesto\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for tok in doc:\n",
        "        if remove_stop and tok.is_stop:\n",
        "            continue\n",
        "        if tok.pos_ not in KEEP_POS:\n",
        "            continue\n",
        "        lemma = tok.lemma_.strip()\n",
        "        if lemma:\n",
        "            tokens.append(lemma)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Creo anche le colonne con la versione lemmatizzata\n",
        "df[\"input_lemma\"]  = df[\"input_clean\"].apply(lemmatize_spacy)\n",
        "df[\"output_lemma\"] = df[\"output_clean\"].apply(lemmatize_spacy)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# ANTEPRIMA\n",
        "# -----------------------------\n",
        "# Qui confronto le tre versioni: testo originale, pulito e lemmatizzato.\n",
        "df[[\"input\", \"input_clean\", \"input_lemma\", \"output\", \"output_clean\", \"output_lemma\"]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6VMy4uQzrsj"
      },
      "source": [
        "### Candidate Answers\n",
        "\n",
        "Nella fase successiva costruisco i **candidate answers** per ogni domanda.  \n",
        "Il dataset *Psychology-6k* infatti fornisce solo la domanda (`input`) e la risposta corretta (`output`), ma non una lista di alternative.  \n",
        "\n",
        "Per poter testare il retrieval, genero quindi per ogni domanda:  \n",
        "- 1 risposta corretta (gold)  \n",
        "- 3 risposte errate scelte casualmente dal resto del dataset  \n",
        "\n",
        "Questa operazione la ripeto **per entrambe le versioni del dataset** (light e lemma), in modo da poter confrontare le prestazioni del sistema in condizioni equivalenti ma con pre-processing diverso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F08oviasyocW"
      },
      "outputs": [],
      "source": [
        "#  Costruzione dei Candidate Answers\n",
        "# In questa fase preparo, per ogni domanda, un set di risposte:\n",
        "# - 1 risposta corretta (gold)\n",
        "# - alcune risposte sbagliate prese a caso dal dataset (negatives)\n",
        "# Questo √® necessario perch√© nel dataset Psychology-6k non ci sono gi√† candidate answers.\n",
        "\n",
        "import random\n",
        "\n",
        "# imposto un seme random fisso cos√¨ i risultati sono riproducibili\n",
        "random.seed(42)\n",
        "\n",
        "def build_candidates(df, question_col, answer_col, k_neg=3):\n",
        "    \"\"\"\n",
        "    Per ogni domanda creo un dizionario con:\n",
        "    - question: la domanda\n",
        "    - gold: la risposta corretta\n",
        "    - candidates: lista con 1 gold + k_neg risposte errate prese a caso dal dataset\n",
        "    \"\"\"\n",
        "    all_answers = df[answer_col].tolist()\n",
        "    N = len(df)\n",
        "    records = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        q = row[question_col]\n",
        "        gold = row[answer_col]\n",
        "\n",
        "        # seleziono k_neg risposte errate diverse dalla gold\n",
        "        negatives = []\n",
        "        while len(negatives) < k_neg:\n",
        "            j = random.randrange(N)\n",
        "            cand = all_answers[j]\n",
        "            if cand != gold and cand not in negatives:\n",
        "                negatives.append(cand)\n",
        "\n",
        "        candidates = [gold] + negatives\n",
        "        random.shuffle(candidates)  # mescolo cos√¨ la gold non √® sempre in prima posizione\n",
        "\n",
        "        records.append({\n",
        "            \"question\": q,\n",
        "            \"gold\": gold,\n",
        "            \"candidates\": candidates\n",
        "        })\n",
        "    return records\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFUmEHyzRRA"
      },
      "source": [
        "### Nota: **confronto**\n",
        "\n",
        "Dopo aver pulito e normalizzato il dataset, ho deciso di preparare **due versioni parallele** dei dati:  \n",
        "\n",
        "1. **Pulizia leggera (light):** testo convertito in minuscole, rimozione del rumore e gestione spazi, mantenendo la punteggiatura utile.  \n",
        "2. **Pulizia + Lemmatizzazione (lemma):** stessa pulizia leggera, ma in pi√π riduzione delle parole alla loro radice tramite SpaCy (es. *running*, *ran*, *runs* ‚Üí *run*).  \n",
        "\n",
        "L‚Äôobiettivo √® confrontare questi due approcci e valutare se la lemmatizzazione porta effettivamente un miglioramento nelle prestazioni del retrieval rispetto alla semplice pulizia leggera.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZbOAQDw0jX9"
      },
      "outputs": [],
      "source": [
        "# Qui preparo i dati per le due modalit√†:\n",
        "# - pulizia leggera (light)\n",
        "# - pulizia + lemmatizzazione (lemma)\n",
        "\n",
        "# Per la pulizia leggera uso input_clean e output_clean\n",
        "records_light = build_candidates(df, question_col=\"input_clean\", answer_col=\"output_clean\", k_neg=3)\n",
        "\n",
        "# Per la pulizia con lemma uso input_lemma e output_lemma\n",
        "records_lemma = build_candidates(df, question_col=\"input_lemma\", answer_col=\"output_lemma\", k_neg=3)\n",
        "\n",
        "# Controllo un esempio per ogni modalit√†\n",
        "print(\"Esempio (pulizia leggera):\")\n",
        "print(records_light[0])\n",
        "\n",
        "print(\"\\nEsempio (pulizia + lemma):\")\n",
        "print(records_lemma[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPaa8WqHAMle"
      },
      "source": [
        "## **Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g3DjNA44iuV"
      },
      "source": [
        "### Generazione Embeddings (light vs lemma)\n",
        "\n",
        "In questa fase trasformo testi (domande e risposte candidate) in vettori numerici usando il modello `thenlper/gte-small`.\n",
        "Creo due insiemi separati di embeddings:\n",
        "- **light** ‚Üí ottenuti dai testi puliti ‚Äúleggeri‚Äù,\n",
        "- **lemma** ‚Üí ottenuti dai testi puliti + lemmatizzati.\n",
        "\n",
        "Scelgo di **deduplicare** i testi prima di codificarli (question + tutti i candidates) per velocizzare e risparmiare memoria.\n",
        "Normalizzo gli embeddings cos√¨ che il **prodotto scalare = similarit√† coseno**, utile per la fase di retrieval.\n",
        "Ho configurato batch_size di 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uQschXN4nQn"
      },
      "outputs": [],
      "source": [
        "#  Embeddings con SentenceTransformers (light vs lemma)\n",
        "\n",
        "# qui importo le librerie necessarie per gli embeddings\n",
        "# --- Embeddings con SentenceTransformers: versione \"pulita\" senza cache globale ---\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "\n",
        "# scelgo il modello con cui lavorare (posso cambiarlo in 1 riga)\n",
        "MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "def make_model(model_name: str = MODEL_NAME):\n",
        "    \"\"\"\n",
        "    Creo l'istanza del modello una sola volta e la ritorno.\n",
        "    Preferisco farlo qui (helper) cos√¨ ho il punto unico dove eventualmente\n",
        "    configurare device/dtype/log ecc.\n",
        "    \"\"\"\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "def encode_texts(texts, model: SentenceTransformer, batch_size: int = 60, normalize: bool = True):\n",
        "    \"\"\"\n",
        "    Trasformo una lista di testi in embeddings usando *l'istanza* del modello\n",
        "    che passo esplicitamente (niente stato globale).\n",
        "    Se normalize=True, gli embedding sono L2-normalizzati e il dot product = coseno.\n",
        "    \"\"\"\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=normalize\n",
        "    )\n",
        "\n",
        "def build_embedding_store(records, model: SentenceTransformer, batch_size: int = 64):\n",
        "    \"\"\"\n",
        "    Costruisco un dizionario {testo -> embedding} per il set di record passato.\n",
        "    - raccolgo tutte le stringhe (domande + candidati)\n",
        "    - deduplica per evitare ricalcoli inutili\n",
        "    - codifico in batch con l'istanza del modello\n",
        "    \"\"\"\n",
        "    uniq_texts = set()\n",
        "    for rec in records:\n",
        "        uniq_texts.add(rec[\"question\"])\n",
        "        for cand in rec[\"candidates\"]:\n",
        "            uniq_texts.add(cand)\n",
        "\n",
        "    uniq_texts = list(uniq_texts)\n",
        "    embs = encode_texts(uniq_texts, model=model, batch_size=batch_size, normalize=True)\n",
        "\n",
        "    store = {txt: vec for txt, vec in zip(uniq_texts, embs)}\n",
        "    return store\n",
        "\n",
        "def store_stats(store, label: str):\n",
        "    \"\"\"\n",
        "    Stampo due numeri per capire 'quanto pesa' lo store:\n",
        "    - quanti vettori\n",
        "    - dimensione del vettore\n",
        "    - stima memoria (float32 ~ 4 byte)\n",
        "    \"\"\"\n",
        "    n_vec = len(store)\n",
        "    dim = next(iter(store.values())).shape[0] if n_vec > 0 else 0\n",
        "    approx_mb = n_vec * dim * 4 / (1024**2)\n",
        "    print(f\"{label}: vettori={n_vec}, dim={dim}, ~mem={approx_mb:.1f} MB\")\n",
        "\n",
        "def quick_check(records, store, label: str):\n",
        "    \"\"\"\n",
        "    Controllo veloce: per il primo record verifico che il prodotto scalare\n",
        "    (coseno se normalizzati) tiri fuori il candidato top.\n",
        "    \"\"\"\n",
        "    print(f\"\\nQuick check ({label})\")\n",
        "    rec = records[0]\n",
        "    q = rec[\"question\"]\n",
        "    cands = rec[\"candidates\"]\n",
        "\n",
        "    qv = store[q]\n",
        "    cv = np.stack([store[c] for c in cands], axis=0)\n",
        "\n",
        "    scores = (cv @ qv).astype(float)\n",
        "    top_idx = int(np.argmax(scores))\n",
        "\n",
        "    print(\"Q    :\", q)\n",
        "    print(\"GOLD :\", rec[\"gold\"])\n",
        "    print(\"TOP  :\", cands[top_idx])\n",
        "    print(\"Scores:\", np.round(scores, 3))\n",
        "\n",
        "# ====== Punto di ingresso: istanzio il modello UNA volta e lo passo dove serve ======\n",
        "\n",
        "# N.B. assumo che records_light e records_lemma siano gi√† costruiti (fase precedente)\n",
        "model = make_model(MODEL_NAME)\n",
        "\n",
        "print(\"‚Üí Costruisco embedding store (light)‚Ä¶\")\n",
        "emb_store_light = build_embedding_store(records_light, model=model)\n",
        "\n",
        "print(\"‚Üí Costruisco embedding store (lemma)‚Ä¶\")\n",
        "emb_store_lemma = build_embedding_store(records_lemma, model=model)\n",
        "\n",
        "# qualche statistica rapida\n",
        "store_stats(emb_store_light, \"Store LIGHT\")\n",
        "store_stats(emb_store_lemma, \"Store LEMMA\")\n",
        "\n",
        "# mini check su un record per modalit√†\n",
        "quick_check(records_light, emb_store_light, \"light\")\n",
        "quick_check(records_lemma, emb_store_lemma, \"lemma\")\n",
        "\n",
        "# salvo su disco per riuso in valutazione (evito ricalcoli)\n",
        "with open(\"emb_store_light.pkl\", \"wb\") as f:\n",
        "    pickle.dump(emb_store_light, f)\n",
        "with open(\"emb_store_lemma.pkl\", \"wb\") as f:\n",
        "    pickle.dump(emb_store_lemma, f)\n",
        "\n",
        "print(\"\\nEmbeddings pronti per retrieval + MRR (senza cache globale, modello passato esplicitamente).\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoSK27WO7XEz"
      },
      "source": [
        "### Nota sul log di download e calcolo embeddings\n",
        "\n",
        "In questa cella Colab prima **scarica il modello `thenlper/gte-small`** da HuggingFace, come si vede dai file elencati (configurazioni, vocabolario, tokenizer e soprattutto `model.safetensors` che contiene i pesi addestrati).  \n",
        "Questa operazione avviene solo la prima volta: nelle esecuzioni successive il modello viene caricato dalla cache.\n",
        "\n",
        "Subito dopo inizia il **calcolo degli embeddings**:  \n",
        "- Ogni frase (domanda + risposte candidate) viene trasformata in un vettore numerico.  \n",
        "- Il log mostra l‚Äôavanzamento a **batch**: ad esempio `64/160` significa 64 batch processati su 160 totali.  \n",
        "- Sono riportati anche la percentuale di completamento, il tempo gi√† trascorso e il tempo stimato rimanente.\n",
        "\n",
        "Queste informazioni servono a monitorare che il modello stia lavorando correttamente e a stimare quanto tempo richiede la generazione degli embeddings sull‚Äôintero dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4Pk1uNAdlT"
      },
      "source": [
        "##**Retrieval e Valutazione (MRR)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl9EjA1J8PmF"
      },
      "source": [
        "### Retrieval + Valutazione (MRR)\n",
        "\n",
        "In questa fase **uso** (cio√® *consumo*) gli embeddings calcolati nella precedente processo per:\n",
        "1) confrontare ogni **domanda** con le sue **candidate answers** via similarit√† coseno,\n",
        "2) calcolare la **Mean Reciprocal Rank (MRR)** per misurare quanto spesso la risposta corretta √® in alto nel ranking.\n",
        "\n",
        "Eseguo tutto **due volte**:\n",
        "- **light** ‚Üí usando gli embeddings generati dai testi con pulizia leggera,\n",
        "- **lemma** ‚Üí usando gli embeddings generati dai testi lemmatizzati.\n",
        "\n",
        "Confronto i due MRR e mostro anche alcuni esempi ‚Äúdifficili‚Äù per analisi qualitativa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yeh7Mrh8MNM"
      },
      "outputs": [],
      "source": [
        "#  FASE 6 ‚Äì Retrieval + MRR (confronto light vs lemma)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# qui definisco una funzione che mi calcola la Reciprocal Rank (RR) per una domanda\n",
        "def reciprocal_rank(gold_idx: int, scores: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    RR = 1 / (posizione della risposta corretta nel ranking, 1-based).\n",
        "    'scores' sono le similarit√† (coseno) tra domanda e candidati.\n",
        "    \"\"\"\n",
        "    order = np.argsort(-scores)             # indici dei candidati ordinati per score decrescente\n",
        "    rank_0_based = np.where(order == gold_idx)[0][0]  # posizione 0-based della gold\n",
        "    return 1.0 / (rank_0_based + 1)         # converto a 1-based\n",
        "\n",
        "# qui definisco una funzione che valuta un intero set (records + store embeddings)\n",
        "def evaluate_mrr(records, emb_store, n_examples_to_log=3):\n",
        "    \"\"\"\n",
        "    Scorro tutte le domande:\n",
        "    - prendo embedding della domanda e dei candidati dallo 'store'\n",
        "    - calcolo similarit√† (dot product perch√© gi√† normalizzati)\n",
        "    - accumulo le RR per mediare in MRR\n",
        "    - raccolgo qualche dettaglio qualitativo (casi difficili)\n",
        "    \"\"\"\n",
        "    rr_list = []\n",
        "    details = []\n",
        "\n",
        "    for rec in records:\n",
        "        q = rec[\"question\"]\n",
        "        cands = rec[\"candidates\"]\n",
        "        gold = rec[\"gold\"]\n",
        "        gold_idx = cands.index(gold)\n",
        "\n",
        "        # recupero i vettori dal magazzino (store) usando il testo come chiave\n",
        "        qv = emb_store[q]                              # shape: (d,)\n",
        "        cv = np.stack([emb_store[c] for c in cands])   # shape: (k, d)\n",
        "\n",
        "        # prodotto scalare = similarit√† coseno (i vettori sono gi√† normalizzati in Fase 5)\n",
        "        scores = (cv @ qv).astype(float)               # shape: (k,)\n",
        "\n",
        "        rr = reciprocal_rank(gold_idx, scores)\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # salvo un po' di info sul caso (mi serve per stampare esempi)\n",
        "        top_idx = int(np.argmax(scores))\n",
        "        details.append({\n",
        "            \"question\": q,\n",
        "            \"gold\": gold,\n",
        "            \"top_pred\": cands[top_idx],\n",
        "            \"rr\": rr,\n",
        "            \"rank_gold\": int(1/rr)\n",
        "        })\n",
        "\n",
        "    mrr = float(np.mean(rr_list))\n",
        "    # ordino i dettagli per mettere in alto i casi peggiori (rank_gold > 1)\n",
        "    details_sorted = sorted(details, key=lambda d: d[\"rank_gold\"], reverse=True)\n",
        "    return mrr, details_sorted[:n_examples_to_log]\n",
        "\n",
        "# qui eseguo la valutazione per entrambe le modalit√† (light e lemma)\n",
        "results = {}\n",
        "\n",
        "print(\" Valuto MRR (light)‚Ä¶\")\n",
        "mrr_light, hard_light = evaluate_mrr(records_light, emb_store_light, n_examples_to_log=3)\n",
        "results[\"light\"] = {\"MRR\": mrr_light, \"hard\": hard_light}\n",
        "\n",
        "print(\" Valuto MRR (lemma)‚Ä¶\")\n",
        "mrr_lemma, hard_lemma = evaluate_mrr(records_lemma, emb_store_lemma, n_examples_to_log=3)\n",
        "results[\"lemma\"] = {\"MRR\": mrr_lemma, \"hard\": hard_lemma}\n",
        "\n",
        "# tabella riassuntiva dei due MRR\n",
        "summary = pd.DataFrame([\n",
        "    {\"mode\": \"light\", \"MRR\": results[\"light\"][\"MRR\"]},\n",
        "    {\"mode\": \"lemma\", \"MRR\": results[\"lemma\"][\"MRR\"]}\n",
        "]).sort_values(\"MRR\", ascending=False)\n",
        "\n",
        "print(\"\\n Confronto MRR\")\n",
        "display(summary)\n",
        "\n",
        "# stampo alcuni esempi ‚Äúdifficili‚Äù per ciascuna modalit√† (gold non al rank 1)\n",
        "def print_hard_examples(mode_label, hard_list):\n",
        "    print(f\"\\nEsempi difficili ‚Äì {mode_label}\")\n",
        "    if not hard_list:\n",
        "        print(\"(Nessun esempio difficile nei primi 3 casi richiesti)\")\n",
        "        return\n",
        "    for ex in hard_list:\n",
        "        print(\"-\" * 70)\n",
        "        print(\"Q   :\", ex[\"question\"])\n",
        "        print(\"GOLD:\", ex[\"gold\"])\n",
        "        print(\"TOP :\", ex[\"top_pred\"])\n",
        "        print(\"RR  :\", round(ex[\"rr\"], 3), f\"(rank_gold = {ex['rank_gold']})\")\n",
        "\n",
        "print_hard_examples(\"light\", results[\"light\"][\"hard\"])\n",
        "print_hard_examples(\"lemma\", results[\"lemma\"][\"hard\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eV8I6Pu9TU4"
      },
      "source": [
        "### Conclusioni sul confronto Light vs Lemma\n",
        "\n",
        "Dopo aver confrontato le due pipeline di pre-processing, i risultati mostrano che:\n",
        "\n",
        "- **Pulizia leggera (light):** ottiene MRR = 0.975  \n",
        "- **Pulizia + Lemmatizzazione (lemma):** ottiene MRR = 0.958  \n",
        "\n",
        "quindi in  entrambi casi il il ranking √® quasi sempre al primo posto.\n",
        "Entrambi i valori sono alti e indicano un sistema di retrieval molto efficace, ma la modalit√† **light √® leggermente superiore**.  \n",
        "La spiegazione √® che i modelli come `gte-small` (SentenceTransformer) sono addestrati su frasi naturali, quindi **beneficiano di mantenere il contesto originale**, inclusa punteggiatura e variazioni morfologiche.  \n",
        "\n",
        "Con la lemmatizzazione, invece, le frasi diventano pi√π ‚Äúpovere‚Äù e astratte (perdono flessioni verbali, tempi e strutture sintattiche), riducendo la ricchezza semantica utile al modello.  \n",
        "\n",
        "**Conclusione:** in questo dataset conviene usare la **pulizia leggera** per gli embeddings, mentre la versione lemmatizzata rimane utile solo come confronto per mostrare l‚Äôimpatto del pre-processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9WYZgwcbBU4"
      },
      "source": [
        "## GUI Retrieval con Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faIXvzzUa7co"
      },
      "outputs": [],
      "source": [
        "#  GUI Retrieval con Gradio (toggle light/lemma, Top-k)\n",
        "# In questa cella preparo una demo interattiva per mostrare il retrieval:\n",
        "# - io scelgo se usare pre-processing \"light\" o \"lemma\"\n",
        "# - inserisco una domanda e scelgo Top-k\n",
        "# - vedo le migliori risposte dal catalogo (tutte le output del dataset)\n",
        "\n",
        "# 1) importo librerie necessarie\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    # !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except ImportError:\n",
        "    # !pip -q install sentence-transformers\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 2) carico una sola volta il modello e lo tengo in cache\n",
        "_model_cache = {}\n",
        "def get_model(name: str = \"thenlper/gte-small\"):\n",
        "    \"\"\"Carico l'encoder una sola volta (cache) per non rallentare la GUI.\"\"\"\n",
        "    if name not in _model_cache:\n",
        "        _model_cache[name] = SentenceTransformer(name)\n",
        "    return _model_cache[name]\n",
        "\n",
        "# 3) utility: encoding normalizzato (cos√¨ il dot product = cosine similarity)\n",
        "def encode_norm(texts, model_name=\"thenlper/gte-small\", batch_size=128):\n",
        "    \"\"\"Genero embeddings normalizzati per un elenco di testi.\"\"\"\n",
        "    model = get_model(model_name)\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=False,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "# 4) preparo i pool di risposte per entrambe le modalit√† (light/lemma)\n",
        "#    uso le colonne gi√† create in Fase 3: df['output_clean'] e df['output_lemma']\n",
        "#    NB: se il dataset contiene duplicati, li rimuovo per velocizzare\n",
        "answer_pool = {}\n",
        "answer_pool[\"light\"] = sorted(set(df[\"output_clean\"].dropna().astype(str).tolist()))\n",
        "answer_pool[\"lemma\"] = sorted(set(df[\"output_lemma\"].dropna().astype(str).tolist()))\n",
        "\n",
        "# 5) pre-calcolo gli embeddings dei pool (prima volta pu√≤ richiedere un po‚Äô)\n",
        "#    cos√¨ a query time calcolo solo l'embedding della domanda (veloce)\n",
        "pool_embs = {}\n",
        "for mode in [\"light\", \"lemma\"]:\n",
        "    print(f\"‚Üª Pre-calcolo embeddings del pool risposte [{mode}] ‚Ä¶\")\n",
        "    pool_embs[mode] = encode_norm(answer_pool[mode], model_name=\"thenlper/gte-small\")\n",
        "\n",
        "# 6) funzione di pre-processing della query, coerente con la modalit√† scelta\n",
        "def preprocess_query(q: str, mode: str) -> str:\n",
        "    \"\"\"Applico lo stesso pre-processing della pipeline al testo della query.\"\"\"\n",
        "    if mode == \"lemma\":\n",
        "        # prima pulizia leggera, poi lemma (coerente con le mie fasi)\n",
        "        q_clean = clean_text_light(q)\n",
        "        q_lemma = lemmatize_spacy(q_clean, remove_stop=True)\n",
        "        return q_lemma\n",
        "    else:\n",
        "        return clean_text_light(q)\n",
        "\n",
        "# 7) funzione principale usata dalla GUI\n",
        "def retrieve(query: str, mode: str, topk: int = 3):\n",
        "    \"\"\"\n",
        "    - applico il pre-processing scelto alla query\n",
        "    - calcolo l'embedding della query\n",
        "    - calcolo la similarit√† coseno con tutte le risposte del pool scelto\n",
        "    - ritorno top-k (risposta, score) + mini nota \"perch√©\"\n",
        "    \"\"\"\n",
        "    if not query or not query.strip():\n",
        "        return [], \"Inserisci una domanda per avviare la ricerca.\"\n",
        "\n",
        "    # applico il pre-processing coerente\n",
        "    q_proc = preprocess_query(query, mode)\n",
        "\n",
        "    # embedding query\n",
        "    qv = encode_norm([q_proc])[0]  # shape: (d,)\n",
        "\n",
        "    # prendo il pool e i suoi embeddings pre-calcolati\n",
        "    cands = answer_pool[mode]\n",
        "    cv = pool_embs[mode]           # shape: (N, d)\n",
        "\n",
        "    # similarit√† coseno = dot product (vettori gi√† normalizzati)\n",
        "    scores = cv @ qv               # shape: (N,)\n",
        "\n",
        "    # ordino e prendo i top-k\n",
        "    topk = int(max(1, min(topk, 10)))\n",
        "    order = np.argsort(-scores)[:topk]\n",
        "\n",
        "    results = []\n",
        "    for idx in order:\n",
        "        results.append([cands[idx], float(scores[idx])])\n",
        "\n",
        "    # mini spiegazione: margine tra top-1 e top-2 (se disponibili)\n",
        "    if len(order) >= 2:\n",
        "        margin = float(scores[order[0]] - scores[order[1]])\n",
        "        why = f\"Margine top1‚Äìtop2: {margin:.3f} (pi√π √® alto, pi√π la scelta √® netta).\"\n",
        "    else:\n",
        "        why = \"Mostro una sola risposta (Top-1).\"\n",
        "\n",
        "    return results, why\n",
        "\n",
        "# 8) costruisco l'interfaccia Gradio\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Retrieval Q‚ÜíA ‚Äì Demo (light vs lemma)\")\n",
        "    gr.Markdown(\n",
        "        \"Seleziono il **pre-processing** (light/lemma), inserisco una **domanda** e scelgo **Top-k**. \"\n",
        "        \"Vedo le migliori risposte dal pool del dataset. Gli score sono similarit√† coseno.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        mode = gr.Radio(choices=[\"light\",\"lemma\"], value=\"light\", label=\"Pre-processing\")\n",
        "        topk = gr.Slider(1, 10, value=3, step=1, label=\"Top-k\")\n",
        "\n",
        "    query = gr.Textbox(label=\"Domanda (in inglese)\")\n",
        "\n",
        "    out_tbl = gr.Dataframe(headers=[\"Risposta\", \"Score\"], datatype=[\"str\",\"number\"], wrap=True)\n",
        "    out_why = gr.Markdown()\n",
        "\n",
        "    btn = gr.Button(\"Cerca\")\n",
        "    btn.click(fn=retrieve, inputs=[query, mode, topk], outputs=[out_tbl, out_why])\n",
        "\n",
        "# 9) lancio la GUI (condivisibile con link pubblico se share=True)\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --clear-output --ClearMetadataPreprocessor.enabled=True \\\n",
        "  --to notebook --inplace \"Psychology-6k Semantic Retrieve:Il match semantico delle risposte in psicologia V2.ipynb\"\n"
      ],
      "metadata": {
        "id": "XOm4Tb0xc3c1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D6VMy4uQzrsj"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNt7BWuVeJ6JtsQ5w6bmAaT"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}