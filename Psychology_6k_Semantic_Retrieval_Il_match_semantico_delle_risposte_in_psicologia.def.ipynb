{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Xqguc04yEO"
      },
      "source": [
        "# **Psychology-6k Semantic Retrieval :Il match semantico delle risposte in psicologia**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il progetto sviluppato realizza un **sistema di retrieval semantico** capace di associare un input (domanda) all‚Äôoutput corretto (risposta) all‚Äôinterno del dataset Psychology-6k, disponibile su HuggingFace.\n",
        "\n",
        "Il flusso √® organizzato come pipeline:\n",
        "\n",
        "* **Preparazione e pulizia dei testi** ‚Äì sono state testate due modalit√†: una light (normalizzazione di base) e una con lemmatizzazione tramite SpaCy. Quest‚Äôultima non era strettamente necessaria per il progetto, ma √® stata integrata a fini didattici.\n",
        "\n",
        "* **Trasformazione in embeddings** ‚Äì tramite il modello thenlper/gte-small di SentenceTransformers.\n",
        "\n",
        "* **Calcolo della similarit√† coseno** ‚Äì per stimare la vicinanza semantica tra domande e risposte candidate.\n",
        "\n",
        "* **Ranking e valutazione** ‚Äì con la metrica Mean Reciprocal Rank (MRR), che misura la posizione media della risposta corretta nella classifica.\n",
        "\n",
        "* **Interfaccia interattiva (Gradio)** ‚Äì per esplorare le domande, confrontare le due pipeline e visualizzare i risultati.\n",
        "\n",
        "*Risultati*\n",
        "Il sistema ha ottenuto valori di **MRR molto elevati: ‚âà0.97** con la pipeline light e ‚âà0.96 con la pipeline lemma, confermando che la risposta corretta viene classificata quasi sempre al primo posto."
      ],
      "metadata": {
        "id": "vREySkrDWRtq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yQEaWCa5JuB"
      },
      "source": [
        "##Caricamento Dataset\n",
        "Ho caricato il dataset **Psychology-6k** direttamente da HuggingFace (`samhog/psychology-6k`).\n",
        "\n",
        "**Dettagli del dataset:**\n",
        "- **Modalities:** Text  \n",
        "- **Format:** JSON  \n",
        "- **Size:** 1K ‚Äì 10K  \n",
        "- **Librerie utilizzabili:** ü§ó Datasets, üêº Pandas\n",
        "\n",
        "In questa fase verifico la struttura dei dati e preparer√≤ il DataFrame per l‚Äôanalisi.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAzuGNJA7IZW"
      },
      "outputs": [],
      "source": [
        "#Caricamento dataset da HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Carico il dataset direttamente da HuggingFace\n",
        "dataset = load_dataset(\"samhog/psychology-6k\")\n",
        "\n",
        "# Converto la parte 'train' in DataFrame Pandas per analisi\n",
        "import pandas as pd\n",
        "df = dataset[\"train\"].to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decido di mostrare alcune righe del Dataset\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "WSW2kN08q7Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Qui mostro, per le prime tre righe del dataset, la coppia domanda (input)\n",
        "# e la relativa risposta corretta (gold answer)\n",
        "for i in range(3):\n",
        "    print(f\"Domanda: {df['input'][i]}\")\n",
        "    print(f\"Risposta gold: {df['output'][i]}\")\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "id": "7pRIgwPzxY2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA: Analisi preliminare Esplorativa del Dataset Psychology- 6K"
      ],
      "metadata": {
        "id": "VWxZfJTmxo4l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSrMvJiMhWrE"
      },
      "outputs": [],
      "source": [
        "# EDA: Analisi Preliminare\n",
        "\n",
        "# Conteggio di record totali\n",
        "print(\"Numero di record totali:\", len(df))\n",
        "\n",
        "#print(\"Numero di colonne totali:\", len(df.columns))\n",
        "#print(df.columns)\n",
        "\n",
        "\n",
        "#Verifico la struttara del dataset se ha split (train/valid/test)\n",
        "print(\"Split presenti:\", dataset.keys())  # es. ['train'] oppure ['train','test']\n",
        "\n",
        "#Valori mancanti (NaN o stringhe vuote)\n",
        "print(\"\\nValori mancanti per colonna:\")\n",
        "print(df.isna().sum())\n",
        "print(\"\\nEsempi di domande vuote:\", df[df[\"input\"].isna()])\n",
        "\n",
        "# Duplicati\n",
        "dup_questions = df.duplicated(subset=[\"input\"]).sum()\n",
        "print(\"\\nDomande duplicate:\", dup_questions)\n",
        "\n",
        "# Trova tutte le righe duplicate (identiche su tutte le colonne)\n",
        "print(df[df.duplicated()].head(10))\n",
        "\n",
        "dup_q_and_ans = df.duplicated(subset=[\"input\", \"output\"]).sum()\n",
        "print(\"Coppie domanda-risposta duplicate:\", dup_q_and_ans)\n",
        "\n",
        "# Distribuzione lunghezze (in caratteri)\n",
        "df[\"q_len_char\"] = df[\"input\"].astype(str).str.len()\n",
        "print(\"\\nStatistiche lunghezze domande (caratteri):\")\n",
        "print(df[\"q_len_char\"].describe())\n",
        "\n",
        "# Distribuzione lunghezze (in token/parole)\n",
        "df[\"q_len_tokens\"] = df[\"input\"].astype(str).str.split().apply(len)\n",
        "print(\"\\nStatistiche lunghezze domande (token):\")\n",
        "print(df[\"q_len_tokens\"].describe())\n",
        "\n",
        "# Visualizzo un campione di domande molto corte o molto lunghe\n",
        "print(\"\\nDomande corte:\")\n",
        "print(df[df[\"q_len_tokens\"] < 5][\"input\"].head(3))\n",
        "print(\"\\nDomande lunghe:\")\n",
        "print(df[df[\"q_len_tokens\"] > 30][\"input\"].head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUJOEvJ1ju6s"
      },
      "source": [
        "Conclusione dell' EDA\n",
        "\n",
        "L‚Äôanalisi esplorativa del dataset *Psychology-6k* ha mostrato che:\n",
        "\n",
        "- **Dimensione**: 5846 record, tutti nell'unico split presente `train.\n",
        "- **Qualit√† dei dati**: nessun valore mancante o domanda vuota, dati quindi completi.  \n",
        "- **Duplicati**: circa 1489 domande duplicate (25%). Questo indica che un quarto delle domande si ripete, probabilmente per coprire pi√π formulazioni di casi simili. Tale ridondanza potrebbe influenzare la fase di retrieval, semplificando eccessivamente la ricerca della risposta. Rilevando dei duplicati o notato che non  ci sono duplicati esatti, ma alcune domande compaiono con risposte diverse. Non lo considero un errore, bens√¨ un limite del dataset: introduce ambiguit√† che possono influenzare la valutazione con MRR.\n",
        "- **Distribuzione lunghezze**: domande di lunghezza media (‚âà13 token), con minimo 4 e massimo 43 token. La variabilit√† √® contenuta, con poche domande molto lunghe che descrivono situazioni complesse (es. depressione, ansia, dipendenze).  \n",
        "- **Struttura**: il dataset non fornisce direttamente una lista di **risposte candidate**. Sono presenti solo la **domanda** (`input`) e la **risposta corretta** (`output`). Per la fase di retrieval sar√† quindi necessario costruire un set di risposte candidate, ad esempio campionando da altre risposte del dataset.\n",
        "\n",
        "**Conclusione:**  \n",
        "Il dataset presenta una buona qualit√† complessiva: √® completo, ben bilanciato nelle lunghezze e senza valori nulli. Gli aspetti critici principali sono:  \n",
        "1. la presenza di domande duplicate (‚âà25%), che richiede attenzione in fase di valutazione;  \n",
        "2. l‚Äôassenza di candidate answers predefinite, che dovranno essere generate artificialmente per testare il sistema di retrieval.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HasCKEvh_tX7"
      },
      "source": [
        "\n",
        "## **Pulizia e normalizzazione del testo**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nA3dcCiBbrW"
      },
      "source": [
        "**Tecniche di pre-processing utilizzate**\n",
        "\n",
        "In questa fase ho applicato due tipi di pre-processing:\n",
        "\n",
        "1. **Pulizia leggera**  \n",
        "   Ho normalizzato il testo portandolo in minuscolo, eliminando caratteri rumorosi (emoji e simboli non alfanumerici) e compattando gli spazi.  \n",
        "   Questa scelta serve a rendere i dati pi√π uniformi senza alterare troppo il contesto, cosa utile per modelli di tipo SBERT che beneficiano di frasi naturali.\n",
        "\n",
        "2. **Lemmatizzazione con SpaCy + filtro POS**  \n",
        "   Ho usato SpaCy per ridurre le parole al loro lemma (es. *running*, *ran* ‚Üí *run*) e ho mantenuto solo le parole di contenuto (sostantivi, verbi, aggettivi e nomi propri).  \n",
        "   In questo modo riduco la variabilit√† morfologica e mi concentro sugli elementi pi√π significativi delle frasi.\n",
        "\n",
        "Ho quindi applicato **due tecniche linguistiche** (lemmatizzazione e POS tagging).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fREWn-l1wYMe"
      },
      "outputs": [],
      "source": [
        "# Pulizia e normalizzazione del testo\n",
        "#importo la libreria\n",
        "\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "# In questa parte definisco due modalit√† di pulizia:\n",
        "# - pulizia leggera: normalizza il testo senza stravolgerlo\n",
        "# - lemmatizzazione: riduce le parole alla radice, cos√¨ riduco la variabilit√† morfologica\n",
        "\n",
        "\n",
        "# Qui creo una whitelist di caratteri che voglio mantenere:\n",
        "_NOISE_CHARS = r\"[^a-zA-Z0-9\\s\\.\\,\\?\\!\\:\\;\\-\\(\\)\\'\\\"]\"\n",
        "\n",
        "def clean_text_light(text: str) -> str:\n",
        "\n",
        "    #Pulizia leggera del testo:\n",
        "    # - converto tutto in minuscolo\n",
        "    #- tolgo caratteri strani (emoji, simboli non utili)\n",
        "    #- compatto gli spazi\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(_NOISE_CHARS, \" \", text)   # tolgo caratteri non ammessi\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip() # elimino spazi multipli\n",
        "    return text\n",
        "\n",
        "# Applico la pulizia leggera alle colonne input e output\n",
        "df[\"input_clean\"]  = df[\"input\"].apply(clean_text_light)\n",
        "df[\"output_clean\"] = df[\"output\"].apply(clean_text_light)\n",
        "\n",
        "\n",
        "\n",
        "# LEMMATIZZAZIONE\n",
        "\n",
        "# In questa parte uso SpaCy per portare le parole al loro lemma\n",
        "\n",
        "# Carico il modello di SpaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "except OSError:\n",
        "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
        "\n",
        "KEEP_POS = {\"NOUN\", \"PROPN\", \"VERB\", \"ADJ\"}  # i POS che considero utili\n",
        "\n",
        "def lemmatize_spacy(text: str, remove_stop: bool = True) -> str:\n",
        "\n",
        "    #Lemmatizzazione con filtro POS:\n",
        "    #- mantengo solo le parole di contenuto\n",
        "    #- tolgo le stopwords\n",
        "\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = []\n",
        "    for tok in doc:\n",
        "        if remove_stop and tok.is_stop:\n",
        "            continue\n",
        "        if tok.pos_ not in KEEP_POS:\n",
        "            continue\n",
        "        lemma = tok.lemma_.strip()\n",
        "        if lemma:\n",
        "            tokens.append(lemma)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Creo anche le colonne con la versione lemmatizzata\n",
        "df[\"input_lemma\"]  = df[\"input_clean\"].apply(lemmatize_spacy)\n",
        "df[\"output_lemma\"] = df[\"output_clean\"].apply(lemmatize_spacy)\n",
        "\n",
        "\n",
        "\n",
        "# Qui confronto le tre versioni: testo originale, pulito e lemmatizzato.\n",
        "df[[\"input\", \"input_clean\", \"input_lemma\", \"output\", \"output_clean\", \"output_lemma\"]].head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6VMy4uQzrsj"
      },
      "source": [
        "### Candidate Answers\n",
        "\n",
        "Nella fase successiva costruisco i **candidate answers** per ogni domanda.  \n",
        "Il dataset *Psychology-6k* infatti fornisce solo la domanda (`input`) e la risposta corretta (`output`), ma non una lista di alternative.  \n",
        "\n",
        "Per poter testare il retrieval, genero quindi per ogni domanda:  \n",
        "- 1 risposta corretta (gold)  \n",
        "- 3 risposte errate scelte casualmente dal resto del dataset  \n",
        "\n",
        "Questa operazione la ripeto **per entrambe le versioni del dataset** (light e lemma), in modo da poter confrontare le prestazioni del sistema in condizioni equivalenti ma con pre-processing diverso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F08oviasyocW"
      },
      "outputs": [],
      "source": [
        "#  Costruzione dei Candidate Answers\n",
        "# In questa fase preparo, per ogni domanda, un set di risposte:\n",
        "# - 1 risposta corretta (gold)\n",
        "# - alcune risposte sbagliate prese a caso dal dataset (negatives)\n",
        "# Questo √® necessario perch√© nel dataset Psychology-6k non ci sono gi√† candidate answers.\n",
        "\n",
        "import random\n",
        "\n",
        "# imposto un seme random fisso cos√¨ i risultati sono riproducibili\n",
        "random.seed(42)\n",
        "\n",
        "def build_candidates(df, question_col, answer_col, k_neg=3):\n",
        "\n",
        "  #  Per ogni domanda creo un dizionario con:\n",
        "  # - question: la domanda\n",
        "  # - gold: la risposta corretta\n",
        "  # - candidates: lista con 1 gold + k_neg risposte errate prese a caso dal\n",
        "  # dataset\n",
        "\n",
        "    all_answers = df[answer_col].tolist()\n",
        "    N = len(df)\n",
        "    records = []\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        q = row[question_col]\n",
        "        gold = row[answer_col]\n",
        "\n",
        "        # seleziono k_neg risposte errate diverse dalla gold\n",
        "        negatives = []\n",
        "        while len(negatives) < k_neg:\n",
        "            j = random.randrange(N)\n",
        "            cand = all_answers[j]\n",
        "            if cand != gold and cand not in negatives:\n",
        "                negatives.append(cand)\n",
        "\n",
        "        candidates = [gold] + negatives\n",
        "        random.shuffle(candidates)  # mescolo cos√¨ la gold non √® sempre in prima posizione\n",
        "\n",
        "        records.append({\n",
        "            \"question\": q,\n",
        "            \"gold\": gold,\n",
        "            \"candidates\": candidates\n",
        "        })\n",
        "    return records\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = build_candidates(df, \"input\", \"output\", k_neg=3)\n",
        "\n",
        "# mostro i primi 3 esempi costruiti\n",
        "for r in records[:3]:\n",
        "    print(\"Domanda:\", r[\"question\"])\n",
        "    print(\"Risposta gold:\", r[\"gold\"])\n",
        "    print(\"Candidates:\", *r[\"candidates\"], sep=\"\\n - \")\n",
        "    print(\"------------\")"
      ],
      "metadata": {
        "id": "BrV_s7tMsa4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzFUmEHyzRRA"
      },
      "source": [
        "### Nota: **confronto**\n",
        "\n",
        "Dopo aver pulito e normalizzato il dataset, ho deciso di preparare **due versioni parallele** dei dati:  \n",
        "\n",
        "1. **Pulizia leggera (light):** testo convertito in minuscole, rimozione del rumore e gestione spazi, mantenendo la punteggiatura utile.  \n",
        "2. **Pulizia + Lemmatizzazione (lemma):** stessa pulizia leggera, ma in pi√π riduzione delle parole alla loro radice tramite SpaCy (es. *running*, *ran*, *runs* ‚Üí *run*).  \n",
        "\n",
        "L‚Äôobiettivo √® confrontare questi due approcci e valutare se la lemmatizzazione porta effettivamente un miglioramento nelle prestazioni del retrieval rispetto alla semplice pulizia leggera.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZbOAQDw0jX9"
      },
      "outputs": [],
      "source": [
        "# Qui preparo i dati per le due modalit√†:\n",
        "# - pulizia leggera (light)\n",
        "# - pulizia + lemmatizzazione (lemma)\n",
        "\n",
        "# Per la pulizia leggera uso input_clean e output_clean\n",
        "records_light = build_candidates(df, question_col=\"input_clean\", answer_col=\"output_clean\", k_neg=3)\n",
        "\n",
        "# Per la pulizia con lemma uso input_lemma e output_lemma\n",
        "records_lemma = build_candidates(df, question_col=\"input_lemma\", answer_col=\"output_lemma\", k_neg=3)\n",
        "\n",
        "# Controllo un esempio per ogni modalit√†\n",
        "print(\"Esempio (pulizia leggera):\")\n",
        "print(records_light[0])\n",
        "\n",
        "print(\"\\nEsempio (pulizia + lemma):\")\n",
        "print(records_lemma[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPaa8WqHAMle"
      },
      "source": [
        "## **Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g3DjNA44iuV"
      },
      "source": [
        "### Generazione Embeddings (light vs lemma)\n",
        "\n",
        "In questa fase trasformo testi (domande e risposte candidate) in vettori numerici usando il modello `thenlper/gte-small`.\n",
        "Creo due insiemi separati di embeddings:\n",
        "- **light** ‚Üí ottenuti dai testi puliti ‚Äúleggeri‚Äù,\n",
        "- **lemma** ‚Üí ottenuti dai testi puliti + lemmatizzati.\n",
        "\n",
        "Scelgo di **deduplicare** i testi prima di codificarli (question + tutti i candidates) per velocizzare e risparmiare memoria.\n",
        "Normalizzo gli embeddings cos√¨ che il **prodotto scalare = similarit√† coseno**, utile per la fase di retrieval.\n",
        "Ho configurato batch_size di 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uQschXN4nQn"
      },
      "outputs": [],
      "source": [
        "#  Embeddings con SentenceTransformers (light vs lemma)\n",
        "\n",
        "# qui importo le librerie necessarie per gli embeddings\n",
        "#Embeddings con SentenceTransformers: versione \"pulita\" senza cache globale\n",
        "\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "\n",
        "# scelgo il modello con cui lavorare\n",
        "MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "def make_model(model_name: str = MODEL_NAME):\n",
        "\n",
        "    #Creo l'istanza del modello una sola volta e la ritorno.\n",
        "    #Preferisco farlo qui (helper) cos√¨ ho il punto unico dove eventualmente\n",
        "    #configurare device/dtype/log ecc.\n",
        "\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "def encode_texts(texts, model: SentenceTransformer, batch_size: int = 60, normalize: bool = True):\n",
        "\n",
        "  #Trasformo una lista di testi in embeddings usando *l'istanza* del modello\n",
        "  #che passo esplicitamente (niente stato globale).\n",
        "  #Se normalize=True, gli embedding sono L2-normalizzati e il dot product = #coseno.\n",
        "\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=True,\n",
        "        normalize_embeddings=normalize\n",
        "    )\n",
        "\n",
        "def build_embedding_store(records, model: SentenceTransformer, batch_size: int = 60):\n",
        "\n",
        "  #Costruisco un dizionario {testo -> embedding} per il set di record passato.\n",
        "  #- raccolgo tutte le stringhe (domande + candidati)\n",
        "  #- deduplica per evitare ricalcoli inutili\n",
        "  #- codifico in batch con l'istanza del modello\n",
        "\n",
        "    uniq_texts = set()\n",
        "    for rec in records:\n",
        "        uniq_texts.add(rec[\"question\"])\n",
        "        for cand in rec[\"candidates\"]:\n",
        "            uniq_texts.add(cand)\n",
        "\n",
        "    uniq_texts = list(uniq_texts)\n",
        "    embs = encode_texts(uniq_texts, model=model, batch_size=batch_size, normalize=True)\n",
        "\n",
        "    store = {txt: vec for txt, vec in zip(uniq_texts, embs)}\n",
        "    return store\n",
        "\n",
        "def store_stats(store, label: str):\n",
        "\n",
        "    #Stampo due numeri per capire 'quanto pesa' lo store:\n",
        "    #- quanti vettori\n",
        "    #- dimensione del vettore\n",
        "    #- stima memoria (float32 ~ 4 byte)\n",
        "\n",
        "    n_vec = len(store)\n",
        "    dim = next(iter(store.values())).shape[0] if n_vec > 0 else 0\n",
        "    approx_mb = n_vec * dim * 4 / (1024**2)\n",
        "    print(f\"{label}: vettori={n_vec}, dim={dim}, ~mem={approx_mb:.1f} MB\")\n",
        "\n",
        "def quick_check(records, store, label: str):\n",
        "\n",
        "    #Controllo veloce: per il primo record verifico che il prodotto scalare\n",
        "    #(coseno se normalizzati) tiri fuori il candidato top.\n",
        "\n",
        "    print(f\"\\nQuick check ({label})\")\n",
        "    rec = records[0]\n",
        "    q = rec[\"question\"]\n",
        "    cands = rec[\"candidates\"]\n",
        "\n",
        "    qv = store[q]\n",
        "    cv = np.stack([store[c] for c in cands], axis=0)\n",
        "\n",
        "    scores = (cv @ qv).astype(float)\n",
        "    top_idx = int(np.argmax(scores))\n",
        "\n",
        "    print(\"Q    :\", q)\n",
        "    print(\"GOLD :\", rec[\"gold\"])\n",
        "    print(\"TOP  :\", cands[top_idx])\n",
        "    print(\"Scores:\", np.round(scores, 3))\n",
        "\n",
        "#Punto di ingresso: istanzio il modello UNA volta e lo passo dove serve\n",
        "\n",
        "model = make_model(MODEL_NAME)\n",
        "\n",
        "print(\"‚Üí Costruisco embedding store (light)‚Ä¶\")\n",
        "emb_store_light = build_embedding_store(records_light, model=model)\n",
        "\n",
        "print(\"‚Üí Costruisco embedding store (lemma)‚Ä¶\")\n",
        "emb_store_lemma = build_embedding_store(records_lemma, model=model)\n",
        "\n",
        "# qualche statistica rapida\n",
        "store_stats(emb_store_light, \"Store LIGHT\")\n",
        "store_stats(emb_store_lemma, \"Store LEMMA\")\n",
        "\n",
        "# mini check su un record per modalit√†\n",
        "quick_check(records_light, emb_store_light, \"light\")\n",
        "quick_check(records_lemma, emb_store_lemma, \"lemma\")\n",
        "\n",
        "# salvo su disco per riuso in valutazione (evito ricalcoli)\n",
        "with open(\"emb_store_light.pkl\", \"wb\") as f:\n",
        "    pickle.dump(emb_store_light, f)\n",
        "with open(\"emb_store_lemma.pkl\", \"wb\") as f:\n",
        "    pickle.dump(emb_store_lemma, f)\n",
        "\n",
        "print(\"\\nEmbeddings pronti per retrieval + MRR \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoSK27WO7XEz"
      },
      "source": [
        "### Nota sul log di download e calcolo embeddings\n",
        "\n",
        "All‚Äôinizio il modello thenlper/gte-small veniva caricato tramite la cache globale di SentenceTransformers: ogni funzione richiamava il modello implicitamente, con maggiore rischio di confusione e ricalcoli.\n",
        "\n",
        "Ho poi cambiato piano, passando a una strategia pi√π chiara:\n",
        "* istanziare il modello una sola volta con make_model,\n",
        "* passarlo esplicitamente alle funzioni che calcolano gli embeddings.\n",
        "\n",
        "In questo modo evitiamo dipendenze globali, rendiamo il codice pi√π trasparente e controlliamo meglio l‚Äôuso della memoria.\n",
        "\n",
        "Subito dopo inizia il calcolo degli embeddings:\n",
        "\n",
        "Ogni frase (domanda + risposte candidate) viene convertita in un vettore numerico.\n",
        "\n",
        "Il log mostra l‚Äôavanzamento in batch: ad esempio 64/160 indica che sono stati processati 64 batch su un totale di 160.\n",
        "\n",
        "Sono riportati anche percentuale di completamento, tempo trascorso e stima del tempo rimanente.\n",
        "\n",
        "Queste informazioni sono utili per monitorare l‚Äôavanzamento del processo e stimare la durata della generazione degli embeddings sull‚Äôintero dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4Pk1uNAdlT"
      },
      "source": [
        "##**Retrieval e Valutazione (MRR)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl9EjA1J8PmF"
      },
      "source": [
        "### Retrieval + Valutazione (MRR)\n",
        "\n",
        "In questa fase consumo gli embeddings calcolati nella precedente processo per:\n",
        "1) confrontare ogni **domanda** con le sue **candidate answers** via similarit√† coseno,\n",
        "2) calcolare la **Mean Reciprocal Rank (MRR)** per misurare quanto spesso la risposta corretta √® in alto nel ranking.\n",
        "\n",
        "Eseguo tutto **due volte**:\n",
        "- **light** ‚Üí usando gli embeddings generati dai testi con pulizia leggera,\n",
        "- **lemma** ‚Üí usando gli embeddings generati dai testi lemmatizzati.\n",
        "\n",
        "Confronto i due MRR e mostro anche alcuni esempi ‚Äúdifficili‚Äù per analisi qualitativa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yeh7Mrh8MNM"
      },
      "outputs": [],
      "source": [
        "# Retrieval + MRR (confronto light vs lemma)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# qui definisco una funzione che mi calcola la RR per una domanda\n",
        "def reciprocal_rank(gold_idx: int, scores: np.ndarray) -> float:\n",
        "\n",
        "    #RR = 1 / (posizione della risposta corretta nel ranking, 1-based).\n",
        "    #'scores' sono le similarit√† (coseno) tra domanda e candidati.\n",
        "\n",
        "    order = np.argsort(-scores)             # indici dei candidati ordinati per score decrescente\n",
        "    rank_0_based = np.where(order == gold_idx)[0][0]  # posizione 0-based della gold\n",
        "    return 1.0 / (rank_0_based + 1)         # converto a 1-based\n",
        "\n",
        "# qui definisco una funzione che valuta un intero set (records + store embeddings)\n",
        "def evaluate_mrr(records, emb_store, n_examples_to_log=3):\n",
        "\n",
        "    #Scorro tutte le domande:\n",
        "    #- prendo embedding della domanda e dei candidati dallo 'store'\n",
        "    #- calcolo similarit√† (dot product perch√© gi√† normalizzati)\n",
        "    #- accumulo le RR per mediare in MRR\n",
        "    #- raccolgo qualche dettaglio qualitativo (casi difficili)\n",
        "\n",
        "    rr_list = []\n",
        "    details = []\n",
        "\n",
        "    for rec in records:\n",
        "        q = rec[\"question\"]\n",
        "        cands = rec[\"candidates\"]\n",
        "        gold = rec[\"gold\"]\n",
        "        gold_idx = cands.index(gold)\n",
        "\n",
        "        # recupero i vettori dal magazzino (store) usando il testo come chiave\n",
        "        qv = emb_store[q]                              # shape: (d,)\n",
        "        cv = np.stack([emb_store[c] for c in cands])   # shape: (k, d)\n",
        "\n",
        "        # prodotto scalare = similarit√† coseno\n",
        "        scores = (cv @ qv).astype(float)               # shape: (k,)\n",
        "\n",
        "        rr = reciprocal_rank(gold_idx, scores)\n",
        "        rr_list.append(rr)\n",
        "\n",
        "        # salvo un po' di info sul caso (mi serve per stampare esempi)\n",
        "        top_idx = int(np.argmax(scores))\n",
        "        details.append({\n",
        "            \"question\": q,\n",
        "            \"gold\": gold,\n",
        "            \"top_pred\": cands[top_idx],\n",
        "            \"rr\": rr,\n",
        "            \"rank_gold\": int(1/rr)\n",
        "        })\n",
        "\n",
        "    mrr = float(np.mean(rr_list))\n",
        "    # ordino i dettagli per mettere in alto i casi peggiori (rank_gold > 1)\n",
        "    details_sorted = sorted(details, key=lambda d: d[\"rank_gold\"], reverse=True)\n",
        "    return mrr, details_sorted[:n_examples_to_log]\n",
        "\n",
        "# qui eseguo la valutazione per entrambe le modalit√† (light e lemma)\n",
        "results = {}\n",
        "\n",
        "print(\" Valuto MRR (light)‚Ä¶\")\n",
        "mrr_light, hard_light = evaluate_mrr(records_light, emb_store_light, n_examples_to_log=3)\n",
        "results[\"light\"] = {\"MRR\": mrr_light, \"hard\": hard_light}\n",
        "\n",
        "print(\" Valuto MRR (lemma)‚Ä¶\")\n",
        "mrr_lemma, hard_lemma = evaluate_mrr(records_lemma, emb_store_lemma, n_examples_to_log=3)\n",
        "results[\"lemma\"] = {\"MRR\": mrr_lemma, \"hard\": hard_lemma}\n",
        "\n",
        "# tabella riassuntiva dei due MRR\n",
        "summary = pd.DataFrame([\n",
        "    {\"mode\": \"light\", \"MRR\": results[\"light\"][\"MRR\"]},\n",
        "    {\"mode\": \"lemma\", \"MRR\": results[\"lemma\"][\"MRR\"]}\n",
        "]).sort_values(\"MRR\", ascending=False)\n",
        "\n",
        "print(\"\\n Confronto MRR\")\n",
        "display(summary)\n",
        "\n",
        "# stampo alcuni esempi ‚Äúdifficili‚Äù per ciascuna modalit√† (gold non al rank 1)\n",
        "def print_hard_examples(mode_label, hard_list):\n",
        "    print(f\"\\nEsempi difficili ‚Äì {mode_label}\")\n",
        "    if not hard_list:\n",
        "        print(\"(Nessun esempio difficile nei primi 3 casi richiesti)\")\n",
        "        return\n",
        "    for ex in hard_list:\n",
        "        print(\"-\" * 70)\n",
        "        print(\"Q   :\", ex[\"question\"])\n",
        "        print(\"GOLD:\", ex[\"gold\"])\n",
        "        print(\"TOP :\", ex[\"top_pred\"])\n",
        "        print(\"RR  :\", round(ex[\"rr\"], 3), f\"(rank_gold = {ex['rank_gold']})\")\n",
        "\n",
        "print_hard_examples(\"light\", results[\"light\"][\"hard\"])\n",
        "print_hard_examples(\"lemma\", results[\"lemma\"][\"hard\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eV8I6Pu9TU4"
      },
      "source": [
        "###**Conclusioni e confronto Light vs Lemma**\n",
        "\n",
        "Dopo aver confrontato le due pipeline di pre-processing, i risultati mostrano che:\n",
        "\n",
        "* Pulizia leggera (light): ottiene MRR = 0.975\n",
        "\n",
        "* Pulizia + Lemmatizzazione (lemma): ottiene MRR = 0.958\n",
        "\n",
        "In entrambi i casi il ranking √® quasi sempre corretto al primo posto. I valori sono molto alti e indicano un sistema di retrieval efficace, con un leggero vantaggio per la modalit√† light.\n",
        "\n",
        "La spiegazione sta nel fatto che modelli come gte-small (SentenceTransformer) sono addestrati su frasi naturali: conservare la forma originale (con punteggiatura e variazioni morfologiche) aiuta il modello a sfruttare meglio il contesto semantico.\n",
        "Con la lemmatizzazione, invece, le frasi risultano pi√π astratte e meno ricche di informazioni, riducendo la capacit√† del modello di distinguere le sfumature di significato.\n",
        "\n",
        "**Limiti del dataset**\n",
        "\n",
        "Come discusso all‚Äôinizio, il dataset Psychology-6k presenta alcune domande duplicate o molto simili, con risposte che variano per coprire pi√π casi. Questo aspetto ha probabilmente influenzato la valutazione della metrica MRR: in presenza di risposte quasi identiche, il sistema pu√≤ ritrovare facilmente la gold answer o una sua variante, ‚Äúgonfiando‚Äù leggermente il punteggio. In altre parole, la metrica pu√≤ risultare ottimistica perch√© non penalizza la ridondanza, restituendo un‚Äôimmagine pi√π rosea delle reali capacit√† del sistema.\n",
        "\n",
        "**Scelta del modello**\n",
        "\n",
        "Avremmo potuto sperimentare con altri SentenceTransformer, ad esempio all-mpnet-base-v2 o bge-small-en-v1.5. Questi modelli, pi√π grandi o pi√π recenti, avrebbero potuto ottenere prestazioni migliori grazie a un embedding pi√π ricco, ma con un costo maggiore in termini di memoria e tempo di calcolo.\n",
        "\n",
        "**Scelta della Metrica**\n",
        "\n",
        "Oltre alla Mean Reciprocal Rank (MRR), avremmo potuto utilizzare la Mean Average Precision (MAP).\n",
        "\n",
        "* MRR valuta solo la posizione della prima risposta corretta nel ranking. Se la gold answer √® in prima posizione, il punteggio √® 1; se √® in terza, vale 1/3, e cos√¨ via. Questo rende la metrica molto sensibile al primo match trovato.\n",
        "\n",
        "* MAP, invece, calcola la precisione media lungo tutto il ranking e la fa sulla media delle query. √à quindi pi√π adatta quando una domanda ha pi√π risposte corrette, perch√© misura non solo se trovi la prima risposta giusta, ma anche come distribuisci tutte le risposte giuste nel ranking.\n",
        "\n",
        "Nel nostro dataset ogni domanda ha una sola gold answer, quindi la MRR appare adeguata. Tuttavia, la presenza di domande duplicate o molto simili con risposte leggermente diverse ha influito sul risultato:\n",
        "\n",
        "* Con la MRR, il sistema ottiene punteggi alti perch√© basta che una delle risposte quasi identiche finisca in alto per avere un reciproco vicino a 1.\n",
        "\n",
        "* Con la MAP, invece, questa ridondanza non avrebbe gonfiato i valori: la metrica avrebbe considerato l‚Äôintero ordinamento e avrebbe dato un peso pi√π equilibrato alla presenza di varianti multiple della stessa risposta.\n",
        "\n",
        "Quindi:\n",
        "* La MRR √® semplice, immediata e sufficiente in presenza di una sola gold answer.\n",
        "\n",
        "* La MAP sarebbe stata pi√π robusta nel gestire i duplicati, ma al costo di maggiore complessit√† e senza un reale beneficio nel nostro dataset specifico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFff5JRLGkvV"
      },
      "source": [
        "#Estensioni\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9WYZgwcbBU4"
      },
      "source": [
        "## GUI Retrieval con Gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# GUI Retrieval con Gradio (light vs lemma)\n",
        "# ---------------------------------------------------------------\n",
        "# Obiettivo dell'interfaccia:\n",
        "# - seleziono il pre-processing (\"light\" o \"lemma\")\n",
        "# - inserisco una domanda e scelgo Top-k\n",
        "# - vedo le migliori risposte dal pool (derivato dal dataset)\n",
        "#\n",
        "# Nota architetturale:\n",
        "\n",
        "# - Gli embeddings del pool vengono pre-calcolati una volta usando la stessa istanza.\n",
        "# - In GUI usiamo `gr.State` per condividere oggetti tra callback\n",
        "#\n",
        "# Prerequisiti:\n",
        "# - Variabili gi√† disponibili: `model`, `df`, `clean_text_light`, `lemmatize_spacy`\n",
        "# - Colonne dataset gi√† pronte: df['output_clean'], df['output_lemma']\n",
        "# ================================================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    # !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "# ---------------------------\n",
        "# Utility: encoding normalizzato\n",
        "# ---------------------------\n",
        "def encode_norm(texts, model, batch_size=128):\n",
        "    \"\"\"Genero embeddings normalizzati (cos√¨ dot product = cosine).\"\"\"\n",
        "    return model.encode(\n",
        "        texts,\n",
        "        batch_size=batch_size,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=False,\n",
        "        normalize_embeddings=True\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Preparo i pool di risposte (light/lemma) dal DataFrame\n",
        "# - deduplica per velocizzare ed evitare bias di duplicati\n",
        "# ------------------------------------------------------\n",
        "answer_pool = {\n",
        "    \"light\": sorted(set(df[\"output_clean\"].dropna().astype(str).tolist())),\n",
        "    \"lemma\": sorted(set(df[\"output_lemma\"].dropna().astype(str).tolist())),\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Pre-calcolo embeddings dei pool con la STESSA istanza `model`\n",
        "# - Questo permette query-time veloce (si embedda solo la query)\n",
        "# ------------------------------------------------------\n",
        "pool_embs = {}\n",
        "for mode in [\"light\", \"lemma\"]:\n",
        "    print(f\"‚Üª Pre-calcolo embeddings del pool risposte [{mode}] ‚Ä¶\")\n",
        "    pool_embs[mode] = encode_norm(answer_pool[mode], model=model)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Pre-processing della query coerente al mode\n",
        "# -----------------------------------------\n",
        "def preprocess_query(q: str, mode: str) -> str:\n",
        "    \"\"\"Applico lo stesso pre-processing della pipeline alla query.\"\"\"\n",
        "    if not q or not isinstance(q, str):\n",
        "        return \"\"\n",
        "    if mode == \"lemma\":\n",
        "        # prima pulizia leggera, poi lemma (come nella pipeline)\n",
        "        q_clean = clean_text_light(q)\n",
        "        q_lemma = lemmatize_spacy(q_clean, remove_stop=True)\n",
        "        return q_lemma\n",
        "    # modalit√† light\n",
        "    return clean_text_light(q)\n",
        "\n",
        "# -----------------------------------------\n",
        "# Funzione principale di retrieval (callback GUI)\n",
        "# -----------------------------------------\n",
        "def retrieve(query: str, mode: str, topk: int, model, pool_embs, answer_pool):\n",
        "    \"\"\"\n",
        "    - Applico pre-processing coerente\n",
        "    - Embedding della query con la STESSA istanza `model`\n",
        "    - Similarit√† coseno con il pool (gi√† normalizzato)\n",
        "    - Ritorno Top-k (risposta, score) + mini nota \"perch√©\"\n",
        "    \"\"\"\n",
        "    if not query or not query.strip():\n",
        "        return [], \"Inserisci una domanda per avviare la ricerca.\"\n",
        "\n",
        "    # Pre-process query\n",
        "    q_proc = preprocess_query(query, mode)\n",
        "\n",
        "    # Embedding query (shape: (d,))\n",
        "    qv = encode_norm([q_proc], model=model)[0]\n",
        "\n",
        "    # Pool selezionato e suoi embeddings\n",
        "    cands = answer_pool[mode]\n",
        "    cv = pool_embs[mode]  # shape: (N, d)\n",
        "\n",
        "    # Similarit√† coseno = dot product (vettori normalizzati)\n",
        "    scores = cv @ qv  # shape: (N,)\n",
        "\n",
        "    # Ordinamento per punteggio decrescente e Top-k (limitato 1..10)\n",
        "    topk = int(max(1, min(topk, 10)))\n",
        "    order = np.argsort(-scores)[:topk]\n",
        "\n",
        "    # Risultati formattati per tabella (risposta, score)\n",
        "    results = [[cands[i], float(scores[i])] for i in order]\n",
        "\n",
        "    # Nota esplicativa: margine Top1‚ÄìTop2 (se disponibili)\n",
        "    if len(order) >= 2:\n",
        "        margin = float(scores[order[0]] - scores[order[1]])\n",
        "        why = f\"Margine top1‚Äìtop2: {margin:.3f} (pi√π √® alto, pi√π la scelta √® netta).\"\n",
        "    else:\n",
        "        why = \"Mostro una sola risposta (Top-1).\"\n",
        "\n",
        "    return results, why\n",
        "\n",
        "# ---------------------------\n",
        "# Costruzione interfaccia GUI\n",
        "# ---------------------------\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Retrieval Q‚ÜíA ‚Äì Demo (light vs lemma)\")\n",
        "    gr.Markdown(\n",
        "        \"Seleziona il **pre-processing** (light/lemma), inserisci una **domanda** e scegli **Top-k**. \"\n",
        "        \"La lista mostra le migliori risposte dal pool del dataset. Gli score sono similarit√† coseno.\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        mode = gr.Radio(choices=[\"light\", \"lemma\"], value=\"light\", label=\"Pre-processing\")\n",
        "        topk = gr.Slider(1, 10, value=3, step=1, label=\"Top-k\")\n",
        "\n",
        "    query = gr.Textbox(label=\"Domanda (in inglese)\")\n",
        "\n",
        "    out_tbl = gr.Dataframe(headers=[\"Risposta\", \"Score\"], datatype=[\"str\", \"number\"], wrap=True)\n",
        "    out_why = gr.Markdown()\n",
        "\n",
        "    # Stati condivisi (no global): passo oggetti alla callback in modo esplicito\n",
        "    # Wrap model in a lambda to prevent Gradio from calling it directly\n",
        "    st_model = gr.State(lambda: model)\n",
        "    st_pool_embs = gr.State(pool_embs)\n",
        "    st_answer_pool = gr.State(answer_pool)\n",
        "\n",
        "    btn = gr.Button(\"Cerca\")\n",
        "    btn.click(\n",
        "        fn=retrieve,\n",
        "        inputs=[query, mode, topk, st_model, st_pool_embs, st_answer_pool],\n",
        "        outputs=[out_tbl, out_why]\n",
        "    )\n",
        "\n",
        "# Avvio GUI (imposta share=True se vuoi link pubblico)\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "UnD3OC5O9k3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Domande di prova per la GUI\n",
        "\n",
        "\n",
        "1. *How can I cope with anxiety during exams?*\n",
        "2. *I feel depressed most of the time, what should I do?*\n",
        "3. *How do I build more confidence in social situations?*\n",
        "4. *I‚Äôm having trouble sleeping because of stress, any advice?*\n",
        "5. *How can I overcome a traumatic experience?*\n",
        "6. *What are healthy ways to deal with anger?*\n",
        "7. *How do I stop overthinking small problems?*\n",
        "8. *I feel lonely even when surrounded by people, why?*\n",
        "\n",
        "---\n",
        "\n",
        "Spiegazione della GUI\n",
        "\n",
        "* Con la **GUI Gradio** non stiamo pi√π esplorando manualmente input/output del dataset.\n",
        "* La GUI lavora solo sulle **risposte del dataset (output)**: queste sono state gi√† pulite (*light* o *lemma*), deduplicate e trasformate in embeddings in anticipo.\n",
        "* Quando scrivi una **nuova domanda** nella GUI:\n",
        "\n",
        "  1. Viene pre-processata in base alla modalit√† scelta.\n",
        "  2. Viene trasformata rapidamente in embedding\n",
        "  3. Questo embedding viene confrontato con quelli delle risposte gi√† calcolati (pool).\n",
        "  4. La GUI ti mostra le Top-k risposte pi√π simili con i loro punteggi di similarit√† coseno.\n",
        "\n",
        "In pratica: **gli output sono fissi e precalcolati**, mentre la domanda che inserisci passa per un processo veloce di embedding al momento della query.\n",
        "\n"
      ],
      "metadata": {
        "id": "WPgU2MdZH2e-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-yQEaWCa5JuB",
        "VWxZfJTmxo4l",
        "HasCKEvh_tX7",
        "D6VMy4uQzrsj",
        "tzFUmEHyzRRA",
        "RPaa8WqHAMle",
        "8g3DjNA44iuV",
        "tl9EjA1J8PmF"
      ],
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPlKhCVFU4gDPYdvGW+neMX"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}